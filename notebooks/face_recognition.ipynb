{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepface import DeepFace\n",
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from environs import Env\n",
    "import numpy as np\n",
    "\n",
    "env = Env()\n",
    "env.read_env('../.env', recurse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Создадим датафрейм для базы изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '../datasets/facescrub_images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_images(path):\n",
    "    \"\"\"Create default dict with img paths\"\"\"\n",
    "    imgs = defaultdict(list)\n",
    "    imgs_list = []\n",
    "    for dir in os.listdir(path):\n",
    "        full_path = os.path.join(path, dir)\n",
    "        for img in os.listdir(full_path):\n",
    "            img_path = os.path.join(full_path, img)         \n",
    "            if img_path.endswith('.png'):\n",
    "                imgs[dir].append(img_path)\n",
    "                imgs_list.append((img_path, dir))\n",
    "    return imgs, imgs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_dict, imgs_list = list_images(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../datasets/facescrub_images/Marilu_Henner/Mar...</td>\n",
       "      <td>Marilu_Henner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../datasets/facescrub_images/Marilu_Henner/Mar...</td>\n",
       "      <td>Marilu_Henner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../datasets/facescrub_images/Marilu_Henner/Mar...</td>\n",
       "      <td>Marilu_Henner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../datasets/facescrub_images/Marilu_Henner/Mar...</td>\n",
       "      <td>Marilu_Henner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../datasets/facescrub_images/Marilu_Henner/Mar...</td>\n",
       "      <td>Marilu_Henner</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            img_path           name\n",
       "0  ../datasets/facescrub_images/Marilu_Henner/Mar...  Marilu_Henner\n",
       "1  ../datasets/facescrub_images/Marilu_Henner/Mar...  Marilu_Henner\n",
       "2  ../datasets/facescrub_images/Marilu_Henner/Mar...  Marilu_Henner\n",
       "3  ../datasets/facescrub_images/Marilu_Henner/Mar...  Marilu_Henner\n",
       "4  ../datasets/facescrub_images/Marilu_Henner/Mar...  Marilu_Henner"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_df = pd.DataFrame(imgs_list, columns=['img_path', 'name'])\n",
    "imgs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name\n",
       "James_Remar        50\n",
       "Lindsay_Hartley    49\n",
       "Kimberlin_Brown    49\n",
       "Sam_Rockwell       49\n",
       "Christel_Khalil    49\n",
       "                   ..\n",
       "Emily_Deschanel    37\n",
       "Brad_Garrett       37\n",
       "Brendan_Fraser     30\n",
       "Adam_Brody         28\n",
       "Bill_Cosby         27\n",
       "Name: count, Length: 80, dtype: int64"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_df['name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "      <th>name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../datasets/facescrub_images/Marilu_Henner/Mar...</td>\n",
       "      <td>Marilu_Henner</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../datasets/facescrub_images/Marilu_Henner/Mar...</td>\n",
       "      <td>Marilu_Henner</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../datasets/facescrub_images/Marilu_Henner/Mar...</td>\n",
       "      <td>Marilu_Henner</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../datasets/facescrub_images/Marilu_Henner/Mar...</td>\n",
       "      <td>Marilu_Henner</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../datasets/facescrub_images/Marilu_Henner/Mar...</td>\n",
       "      <td>Marilu_Henner</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            img_path           name  label\n",
       "0  ../datasets/facescrub_images/Marilu_Henner/Mar...  Marilu_Henner     49\n",
       "1  ../datasets/facescrub_images/Marilu_Henner/Mar...  Marilu_Henner     49\n",
       "2  ../datasets/facescrub_images/Marilu_Henner/Mar...  Marilu_Henner     49\n",
       "3  ../datasets/facescrub_images/Marilu_Henner/Mar...  Marilu_Henner     49\n",
       "4  ../datasets/facescrub_images/Marilu_Henner/Mar...  Marilu_Henner     49"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "imgs_df['label'] = encoder.fit_transform(imgs_df['name'])\n",
    "imgs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Извлечение по несколько фото для каждого объекта для загрузки в базу данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "      <th>name</th>\n",
       "      <th>label</th>\n",
       "      <th>obj_row</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../datasets/facescrub_images/Marilu_Henner/Mar...</td>\n",
       "      <td>Marilu_Henner</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../datasets/facescrub_images/Marilu_Henner/Mar...</td>\n",
       "      <td>Marilu_Henner</td>\n",
       "      <td>49</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../datasets/facescrub_images/Marilu_Henner/Mar...</td>\n",
       "      <td>Marilu_Henner</td>\n",
       "      <td>49</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../datasets/facescrub_images/Marilu_Henner/Mar...</td>\n",
       "      <td>Marilu_Henner</td>\n",
       "      <td>49</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../datasets/facescrub_images/Marilu_Henner/Mar...</td>\n",
       "      <td>Marilu_Henner</td>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            img_path           name  label  \\\n",
       "0  ../datasets/facescrub_images/Marilu_Henner/Mar...  Marilu_Henner     49   \n",
       "1  ../datasets/facescrub_images/Marilu_Henner/Mar...  Marilu_Henner     49   \n",
       "2  ../datasets/facescrub_images/Marilu_Henner/Mar...  Marilu_Henner     49   \n",
       "3  ../datasets/facescrub_images/Marilu_Henner/Mar...  Marilu_Henner     49   \n",
       "4  ../datasets/facescrub_images/Marilu_Henner/Mar...  Marilu_Henner     49   \n",
       "\n",
       "   obj_row  \n",
       "0        1  \n",
       "1        2  \n",
       "2        3  \n",
       "3        4  \n",
       "4        5  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMGS_IN_DB_COUNT = 40\n",
    "\n",
    "imgs_df['obj_row'] = imgs_df.groupby(['label']).cumcount() + 1\n",
    "imgs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_db_df = imgs_df[imgs_df['obj_row'] < IMGS_IN_DB_COUNT]\n",
    "imgs_test_df = imgs_df[imgs_df['obj_row'] >= IMGS_IN_DB_COUNT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Загрузка эмбеддингов изображений в БД"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2 as pg\n",
    "\n",
    "conn=pg.connect(\n",
    "    dbname='maska',\n",
    "      user='postgres',\n",
    "        password= env.str('PSQL_PASSWORD'),\n",
    "          host='localhost',\n",
    "          port='5432' )\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим таблицу в БД"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECOGNITION_MODEL = 'ArcFace'\n",
    "DETECT_MODEL = 'opencv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(f'DROP TABLE IF EXISTS faces_{RECOGNITION_MODEL}')\n",
    "cursor.execute(f'CREATE TABLE IF NOT EXISTS faces_{RECOGNITION_MODEL} (id bigserial PRIMARY KEY, name varchar(30), label int4, row int4, embedding vector(512))')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сформируем эмбеддинги изображений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb_list_from_df(img_df: pd.DataFrame, embedding_model, backend_model) -> list:\n",
    "    emb_list = []\n",
    "    errors_list = []\n",
    "    for path, name, label, row in img_df.values:\n",
    "        try:\n",
    "            faces_obj = DeepFace.represent(cv2.imread(path), model_name=embedding_model, detector_backend=backend_model)\n",
    "            embedding = np.array(faces_obj[0]['embedding'])\n",
    "            emb_list.append((name, label, row, embedding))\n",
    "        except:\n",
    "            print(\"Can't detect face\", name, row)\n",
    "            errors_list.append((name, path, row))\n",
    "        \n",
    "    return emb_list, errors_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't detect face Brianna_Brown 17\n",
      "Can't detect face Adrienne_Barbeau 1\n",
      "Can't detect face Adrienne_Barbeau 4\n",
      "Can't detect face Adrienne_Barbeau 5\n",
      "Can't detect face Adrienne_Barbeau 7\n",
      "Can't detect face Adrienne_Barbeau 10\n",
      "Can't detect face Adrienne_Barbeau 12\n",
      "Can't detect face Adrienne_Barbeau 17\n",
      "Can't detect face Adrienne_Barbeau 18\n",
      "Can't detect face Adrienne_Barbeau 23\n",
      "Can't detect face Adrienne_Barbeau 30\n",
      "Can't detect face Adrienne_Barbeau 32\n",
      "Can't detect face Adrienne_Barbeau 37\n",
      "Can't detect face J.K._Simmons 1\n",
      "Can't detect face J.K._Simmons 20\n",
      "Can't detect face J.K._Simmons 21\n",
      "Can't detect face J.K._Simmons 23\n",
      "Can't detect face J.K._Simmons 28\n",
      "Can't detect face J.K._Simmons 29\n",
      "Can't detect face J.K._Simmons 30\n",
      "Can't detect face Geoffrey_Rush 3\n",
      "Can't detect face Geoffrey_Rush 7\n",
      "Can't detect face Geoffrey_Rush 9\n",
      "Can't detect face Geoffrey_Rush 11\n",
      "Can't detect face Geoffrey_Rush 14\n",
      "Can't detect face Geoffrey_Rush 17\n",
      "Can't detect face Geoffrey_Rush 18\n",
      "Can't detect face Geoffrey_Rush 21\n",
      "Can't detect face Geoffrey_Rush 23\n",
      "Can't detect face Geoffrey_Rush 29\n",
      "Can't detect face Geoffrey_Rush 33\n",
      "Can't detect face Geoffrey_Rush 36\n",
      "Can't detect face Alec_Baldwin 17\n",
      "Can't detect face Justin_Timberlake 18\n",
      "Can't detect face Robin_Williams 4\n",
      "Can't detect face Robin_Williams 7\n",
      "Can't detect face Robin_Williams 31\n",
      "Can't detect face Robin_Williams 34\n",
      "Can't detect face Robin_Williams 37\n",
      "Can't detect face Robin_Williams 38\n",
      "Can't detect face Peggy_McCay 14\n",
      "Can't detect face Peggy_McCay 15\n",
      "Can't detect face Peggy_McCay 20\n",
      "Can't detect face Peggy_McCay 23\n",
      "Can't detect face Peggy_McCay 39\n",
      "Can't detect face Brendan_Fraser 29\n",
      "Can't detect face Melina_Kanakaredes 8\n",
      "Can't detect face Edie_Falco 17\n",
      "Can't detect face Edie_Falco 24\n",
      "Can't detect face Orlando_Bloom 9\n",
      "Can't detect face Orlando_Bloom 11\n",
      "Can't detect face Orlando_Bloom 14\n",
      "Can't detect face Orlando_Bloom 28\n",
      "Can't detect face Courteney_Cox 12\n",
      "Can't detect face Christopher_Reeve 13\n",
      "Can't detect face Christopher_Reeve 15\n",
      "Can't detect face Christopher_Reeve 24\n",
      "Can't detect face Chris_Evans 39\n",
      "Can't detect face Bill_Cosby 4\n",
      "Can't detect face Bill_Cosby 5\n",
      "Can't detect face Bill_Cosby 11\n",
      "Can't detect face Bill_Cosby 12\n",
      "Can't detect face Bill_Cosby 26\n",
      "Can't detect face Kiefer_Sutherland 34\n",
      "Can't detect face Karl_Urban 16\n",
      "Can't detect face Victoria_Justice 11\n",
      "Can't detect face Robert_Knepper 33\n",
      "Can't detect face Patrick_Swayze 3\n",
      "Can't detect face Patrick_Swayze 20\n",
      "Can't detect face Paul_Bettany 8\n",
      "Can't detect face Paul_Bettany 22\n",
      "Can't detect face Paul_Bettany 24\n",
      "Can't detect face Paul_Bettany 39\n",
      "Can't detect face Jackie_Chan 8\n",
      "Can't detect face Jackie_Chan 13\n",
      "Can't detect face Bobbie_Eakes 4\n",
      "Can't detect face Bobbie_Eakes 32\n",
      "Can't detect face Alley_Mills 24\n",
      "Can't detect face Mark_Wahlberg 14\n",
      "Can't detect face Mark_Wahlberg 20\n",
      "Can't detect face Heath_Ledger 24\n",
      "Can't detect face Heath_Ledger 27\n",
      "Can't detect face Heath_Ledger 37\n",
      "Can't detect face Didi_Conn 3\n",
      "Can't detect face Didi_Conn 5\n",
      "Can't detect face Didi_Conn 6\n",
      "Can't detect face Didi_Conn 10\n",
      "Can't detect face Didi_Conn 16\n",
      "Can't detect face Didi_Conn 25\n",
      "Can't detect face Didi_Conn 29\n",
      "Can't detect face Didi_Conn 39\n",
      "Can't detect face James_Remar 9\n",
      "Can't detect face James_Remar 10\n",
      "Can't detect face James_Remar 18\n",
      "Can't detect face James_Remar 23\n",
      "Can't detect face Matthew_Broderick 5\n",
      "Can't detect face Matthew_Broderick 9\n",
      "Can't detect face Matthew_Broderick 22\n",
      "Can't detect face Matthew_Broderick 39\n",
      "Can't detect face Shia_LaBeouf 5\n",
      "Can't detect face Shia_LaBeouf 8\n",
      "Can't detect face Shia_LaBeouf 10\n",
      "Can't detect face Shia_LaBeouf 27\n",
      "Can't detect face Robert_Duvall 10\n",
      "Can't detect face Robert_Duvall 13\n",
      "Can't detect face Robert_Duvall 16\n",
      "Can't detect face Robert_Duvall 34\n",
      "Can't detect face Sarah_Hyland 18\n",
      "Can't detect face Sarah_Hyland 20\n",
      "Can't detect face Joanna_Kerns 12\n",
      "Can't detect face Joanna_Kerns 36\n",
      "Can't detect face Nadia_Bjorlin 24\n",
      "Can't detect face Ilene_Kristen 29\n",
      "Can't detect face Richard_Madden 7\n",
      "Can't detect face Richard_Madden 32\n",
      "Can't detect face Richard_Madden 39\n",
      "Can't detect face Ryan_Phillippe 33\n",
      "Can't detect face Stana_Katic 7\n",
      "Can't detect face Stana_Katic 22\n",
      "Can't detect face Daniel_Radcliffe 7\n",
      "Can't detect face Daniel_Radcliffe 8\n",
      "Can't detect face Julie_Bowen 12\n",
      "Can't detect face Julie_Bowen 32\n",
      "Can't detect face Morena_Baccarin 26\n",
      "Can't detect face Morena_Baccarin 35\n",
      "Can't detect face Robert_Redford 1\n",
      "Can't detect face Robert_Redford 6\n",
      "Can't detect face Robert_Redford 7\n",
      "Can't detect face Robert_Redford 8\n",
      "Can't detect face Robert_Redford 37\n",
      "Can't detect face Robert_Redford 38\n",
      "Can't detect face Rebecca_Budig 4\n",
      "Can't detect face Rebecca_Budig 18\n",
      "Can't detect face Pamela_Anderson 5\n",
      "Can't detect face Pamela_Anderson 7\n",
      "Can't detect face Pamela_Anderson 8\n",
      "Can't detect face Pamela_Anderson 13\n",
      "Can't detect face Pamela_Anderson 15\n",
      "Can't detect face Pamela_Anderson 23\n",
      "Can't detect face Pamela_Anderson 26\n",
      "Can't detect face Pamela_Anderson 28\n",
      "Can't detect face Pamela_Anderson 37\n",
      "Can't detect face Pamela_Anderson 38\n",
      "Can't detect face Martin_Henderson 23\n",
      "Can't detect face Sam_Rockwell 26\n",
      "Can't detect face Tempestt_Bledsoe 10\n",
      "Can't detect face Tempestt_Bledsoe 17\n",
      "Can't detect face Tempestt_Bledsoe 29\n",
      "Can't detect face Jeremy_Irons 17\n",
      "Can't detect face Jeremy_Irons 36\n",
      "Can't detect face Jim_Carrey 20\n",
      "Can't detect face Farrah_Fawcett 3\n",
      "Can't detect face Farrah_Fawcett 7\n",
      "Can't detect face Farrah_Fawcett 17\n",
      "Can't detect face Farrah_Fawcett 21\n",
      "На векторизацию отправлено 3080 изображений. Изображений, где не обнаружены лица: 155 \n"
     ]
    }
   ],
   "source": [
    "imgs_db_embeddings, errors_list = get_emb_list_from_df(imgs_db_df, embedding_model=RECOGNITION_MODEL, backend_model=DETECT_MODEL)\n",
    "print(f'На векторизацию отправлено {imgs_db_df.shape[0]} изображений. Изображений, где не обнаружены лица: {len(errors_list)} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество изображений, которые не попали в БД по каждому объекту:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[147], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[0;32m----> 3\u001b[0m names \u001b[38;5;241m=\u001b[39m [name \u001b[38;5;28;01mfor\u001b[39;00m name, path, row \u001b[38;5;129;01min\u001b[39;00m errors_list]\n\u001b[1;32m      5\u001b[0m counts \u001b[38;5;241m=\u001b[39m Counter(names)\n\u001b[1;32m      6\u001b[0m counts\n",
      "Cell \u001b[0;32mIn[147], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[0;32m----> 3\u001b[0m names \u001b[38;5;241m=\u001b[39m [name \u001b[38;5;28;01mfor\u001b[39;00m name, path, row \u001b[38;5;129;01min\u001b[39;00m errors_list]\n\u001b[1;32m      5\u001b[0m counts \u001b[38;5;241m=\u001b[39m Counter(names)\n\u001b[1;32m      6\u001b[0m counts\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "names = [name for name, path, row in errors_list]\n",
    "\n",
    "counts = Counter(names)\n",
    "counts\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запись значений в БД"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, label, row, embedding in imgs_db_embeddings:\n",
    "    embedding = list(embedding)\n",
    "    cursor.execute(f'INSERT INTO faces_{RECOGNITION_MODEL} (name, label, row, embedding) VALUES (%s, %s, %s, %s)', (name, label, row, embedding))\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предсказываем людей по тестовым фото, сравнивая фото с изображениями в БД"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним эмбеддинги тестовых изображений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't detect face Marilu_Henner 40\n",
      "Can't detect face Adrienne_Barbeau 45\n",
      "Can't detect face Geoffrey_Rush 41\n",
      "Can't detect face Geoffrey_Rush 45\n",
      "Can't detect face Melina_Kanakaredes 44\n",
      "Can't detect face Chris_Evans 42\n",
      "Can't detect face Karl_Urban 42\n",
      "Can't detect face Robert_Knepper 44\n",
      "Can't detect face Patrick_Swayze 41\n",
      "Can't detect face Paul_Bettany 42\n",
      "Can't detect face Jackie_Chan 41\n",
      "Can't detect face Jackie_Chan 47\n",
      "Can't detect face Bobbie_Eakes 42\n",
      "Can't detect face Heath_Ledger 40\n",
      "Can't detect face Heath_Ledger 43\n",
      "Can't detect face Heath_Ledger 45\n",
      "Can't detect face Didi_Conn 42\n",
      "Can't detect face James_Remar 44\n",
      "Can't detect face James_Remar 48\n",
      "Can't detect face Shia_LaBeouf 44\n",
      "Can't detect face Joanna_Kerns 42\n",
      "Can't detect face Joanna_Kerns 43\n",
      "Can't detect face Ilene_Kristen 43\n",
      "Can't detect face Christian_Bale 41\n",
      "Can't detect face Robert_Redford 41\n",
      "Can't detect face Robert_Redford 42\n",
      "Can't detect face Christine_Lakin 42\n",
      "Can't detect face James_Marsden 42\n",
      "Can't detect face James_Marsden 46\n",
      "Can't detect face James_Marsden 47\n",
      "Can't detect face Andrea_Bowen 42\n",
      "Can't detect face Farrah_Fawcett 42\n",
      "На векторизацию отправлено 450 изображений. Изображений, где не обнаружены лица: 32 \n"
     ]
    }
   ],
   "source": [
    "imgs_test_embeddings, errors_test_list = get_emb_list_from_df(imgs_test_df, embedding_model=RECOGNITION_MODEL, backend_model=DETECT_MODEL)\n",
    "print(f'На векторизацию отправлено {imgs_test_df.shape[0]} изображений. Изображений, где не обнаружены лица: {len(errors_test_list)} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество изображений из тестовой выборки, где не удалось обнаружить лицо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_test_df.shape[0] - len(imgs_test_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдем в БД ближайшего человека"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "418"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = []\n",
    "for name, label, row, embedding in imgs_test_embeddings:\n",
    "    embedding = list(embedding)\n",
    "    cursor.execute(f'SELECT * FROM faces_{RECOGNITION_MODEL} ORDER BY embedding <=> %s::vector LIMIT 1', (embedding,))\n",
    "    predictions.append(cursor.fetchone())\n",
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49, 'Marilu_Henner')"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_labels_names = [(label, name) for id, name, label, row, embedding in predictions]\n",
    "predictions_labels_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat((pd.DataFrame([(name, label) for name, label, row, emb in imgs_test_embeddings], columns=['name', 'label']), pd.DataFrame(predictions_labels_names, columns=['label_pred', 'name_pred'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>label</th>\n",
       "      <th>label_pred</th>\n",
       "      <th>name_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Marilu_Henner</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>Marilu_Henner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Marilu_Henner</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>Marilu_Henner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Marilu_Henner</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>Marilu_Henner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Marilu_Henner</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>Marilu_Henner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Marilu_Henner</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>Marilu_Henner</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            name  label  label_pred      name_pred\n",
       "0  Marilu_Henner     49          49  Marilu_Henner\n",
       "1  Marilu_Henner     49          49  Marilu_Henner\n",
       "2  Marilu_Henner     49          49  Marilu_Henner\n",
       "3  Marilu_Henner     49          49  Marilu_Henner\n",
       "4  Marilu_Henner     49          49  Marilu_Henner"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Измерим качество предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_DetectModel:opencv, RecModel: ArcFace:, images in DB: 40  99.28229665071771\n",
      "Precision_DetectModel:opencv, RecModel: ArcFace: : 99.58798511430092\n",
      "recall_DetectModel:opencv, RecModel: ArcFace: : 99.28229665071771\n",
      "f1_DetectModel:opencv, RecModel: ArcFace: : 99.38024417337672\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics imporst accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "actuals = final_df['label'].tolist()\n",
    "predictions = final_df['label_pred'].tolist()\n",
    "accuracy = 100*accuracy_score(actuals, predictions)\n",
    "precision = 100*precision_score(actuals, predictions, average='weighted')\n",
    "recall = 100*recall_score(actuals, predictions, average='weighted')\n",
    "f1 = 100*f1_score(actuals, predictions, average='weighted')\n",
    "print(f'Accuracy_DetectModel:{DETECT_MODEL}, RecModel: {RECOGNITION_MODEL}:, images in DB: {IMGS_IN_DB_COUNT} ', accuracy)\n",
    "print(f'Precision_DetectModel:{DETECT_MODEL}, RecModel: {RECOGNITION_MODEL}: :', precision)\n",
    "print(f'recall_DetectModel:{DETECT_MODEL}, RecModel: {RECOGNITION_MODEL}: :', recall)\n",
    "print(f'f1_DetectModel:{DETECT_MODEL}, RecModel: {RECOGNITION_MODEL}: :', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.python.keras.engine import training\n",
    "from tensorflow.keras.layers import (\n",
    "    ZeroPadding2D,\n",
    "    Input,\n",
    "    Conv2D,\n",
    "    BatchNormalization,\n",
    "    PReLU,\n",
    "    Add,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    Dense,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(\n",
    "    url=\"https://github.com/serengil/deepface_models/releases/download/v1.0/arcface_weights.h5\",\n",
    ") -> Model:\n",
    "    \"\"\"\n",
    "    Construct ArcFace model, download its weights and load\n",
    "    Returns:\n",
    "        model (Model)\n",
    "    \"\"\"\n",
    "    base_model = ResNet34()\n",
    "    inputs = base_model.inputs[0]\n",
    "    arcface_model = base_model.outputs[0]\n",
    "    arcface_model = BatchNormalization(momentum=0.9, epsilon=2e-5)(arcface_model)\n",
    "    arcface_model = Dropout(0.4)(arcface_model)\n",
    "    arcface_model = Flatten()(arcface_model)\n",
    "    arcface_model = Dense(512, activation=None, use_bias=True, kernel_initializer=\"glorot_normal\")(\n",
    "        arcface_model\n",
    "    )\n",
    "    embedding = BatchNormalization(momentum=0.9, epsilon=2e-5, name=\"embedding\", scale=True)(\n",
    "        arcface_model\n",
    "    )\n",
    "    model = Model(inputs, embedding, name=base_model.name)\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # check the availability of pre-trained weights\n",
    "\n",
    "    home = folder_utils.get_deepface_home()\n",
    "\n",
    "    file_name = \"../models/arcface_weights.h5\"\n",
    "\n",
    "    # if os.path.isfile(file_name) != True:\n",
    "\n",
    "    #     logger.info(f\"{file_name} will be downloaded to {file_name}\")\n",
    "    #     gdown.download(url, output, quiet=False)\n",
    "\n",
    "    # ---------------------------------------\n",
    "\n",
    "    model.load_weights(file_name)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet34() -> Model:\n",
    "    \"\"\"\n",
    "    ResNet34 model\n",
    "    Returns:\n",
    "        model (Model)\n",
    "    \"\"\"\n",
    "    img_input = Input(shape=(112, 112, 3))\n",
    "\n",
    "    x = ZeroPadding2D(padding=1, name=\"conv1_pad\")(img_input)\n",
    "    x = Conv2D(\n",
    "        64, 3, strides=1, use_bias=False, kernel_initializer=\"glorot_normal\", name=\"conv1_conv\"\n",
    "    )(x)\n",
    "    x = BatchNormalization(axis=3, epsilon=2e-5, momentum=0.9, name=\"conv1_bn\")(x)\n",
    "    x = PReLU(shared_axes=[1, 2], name=\"conv1_prelu\")(x)\n",
    "    x = stack_fn(x)\n",
    "\n",
    "    model = training.Model(img_input, x, name=\"ResNet34\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block1(x, filters, kernel_size=3, stride=1, conv_shortcut=True, name=None):\n",
    "    bn_axis = 3\n",
    "\n",
    "    if conv_shortcut:\n",
    "        shortcut = Conv2D(\n",
    "            filters,\n",
    "            1,\n",
    "            strides=stride,\n",
    "            use_bias=False,\n",
    "            kernel_initializer=\"glorot_normal\",\n",
    "            name=name + \"_0_conv\",\n",
    "        )(x)\n",
    "        shortcut = BatchNormalization(\n",
    "            axis=bn_axis, epsilon=2e-5, momentum=0.9, name=name + \"_0_bn\"\n",
    "        )(shortcut)\n",
    "    else:\n",
    "        shortcut = x\n",
    "\n",
    "    x = BatchNormalization(axis=bn_axis, epsilon=2e-5, momentum=0.9, name=name + \"_1_bn\")(x)\n",
    "    x = ZeroPadding2D(padding=1, name=name + \"_1_pad\")(x)\n",
    "    x = Conv2D(\n",
    "        filters,\n",
    "        3,\n",
    "        strides=1,\n",
    "        kernel_initializer=\"glorot_normal\",\n",
    "        use_bias=False,\n",
    "        name=name + \"_1_conv\",\n",
    "    )(x)\n",
    "    x = BatchNormalization(axis=bn_axis, epsilon=2e-5, momentum=0.9, name=name + \"_2_bn\")(x)\n",
    "    x = PReLU(shared_axes=[1, 2], name=name + \"_1_prelu\")(x)\n",
    "\n",
    "    x = ZeroPadding2D(padding=1, name=name + \"_2_pad\")(x)\n",
    "    x = Conv2D(\n",
    "        filters,\n",
    "        kernel_size,\n",
    "        strides=stride,\n",
    "        kernel_initializer=\"glorot_normal\",\n",
    "        use_bias=False,\n",
    "        name=name + \"_2_conv\",\n",
    "    )(x)\n",
    "    x = BatchNormalization(axis=bn_axis, epsilon=2e-5, momentum=0.9, name=name + \"_3_bn\")(x)\n",
    "\n",
    "    x = Add(name=name + \"_add\")([shortcut, x])\n",
    "    return x\n",
    "\n",
    "\n",
    "def stack1(x, filters, blocks, stride1=2, name=None):\n",
    "    x = block1(x, filters, stride=stride1, name=name + \"_block1\")\n",
    "    for i in range(2, blocks + 1):\n",
    "        x = block1(x, filters, conv_shortcut=False, name=name + \"_block\" + str(i))\n",
    "    return x\n",
    "\n",
    "\n",
    "def stack_fn(x):\n",
    "    x = stack1(x, 64, 3, name=\"conv2\")\n",
    "    x = stack1(x, 128, 4, name=\"conv3\")\n",
    "    x = stack1(x, 256, 6, name=\"conv4\")\n",
    "    return stack1(x, 512, 3, name=\"conv5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Any, Union, List, Tuple\n",
    "\n",
    "class FacialRecognition(ABC):\n",
    "    model: Union[Model, Any]\n",
    "    model_name: str\n",
    "    input_shape: Tuple[int, int]\n",
    "    output_shape: int\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def find_embeddings(self, img: np.ndarray) -> List[float]:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcFaceClient(FacialRecognition):\n",
    "    \"\"\"\n",
    "    ArcFace model class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = load_model()\n",
    "        self.model_name = \"ArcFace\"\n",
    "        self.input_shape = (112, 112)\n",
    "        self.output_shape = 512\n",
    "\n",
    "    def find_embeddings(self, img: np.ndarray) -> List[float]:\n",
    "        \"\"\"\n",
    "        find embeddings with ArcFace model\n",
    "        Args:\n",
    "            img (np.ndarray): pre-loaded image in BGR\n",
    "        Returns\n",
    "            embeddings (list): multi-dimensional vector\n",
    "        \"\"\"\n",
    "        # model.predict causes memory issue when it is called in a for loop\n",
    "        # embedding = model.predict(img, verbose=0)[0].tolist()\n",
    "        return self.model(img, training=False).numpy()[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No model config found in the file at <tensorflow.python.platform.gfile.GFile object at 0x7f977f935120>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[132], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../models/arcface_weights.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/opencv/.venv/lib/python3.10/site-packages/keras/src/saving/saving_api.py:262\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m    255\u001b[0m         filepath,\n\u001b[1;32m    256\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[1;32m    258\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[1;32m    259\u001b[0m     )\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_sm_saving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/opencv/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/code/opencv/.venv/lib/python3.10/site-packages/keras/src/saving/legacy/hdf5_format.py:197\u001b[0m, in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    195\u001b[0m model_config \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mattrs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_config\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo model config found in the file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model_config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecode\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    201\u001b[0m     model_config \u001b[38;5;241m=\u001b[39m model_config\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: No model config found in the file at <tensorflow.python.platform.gfile.GFile object at 0x7f977f935120>."
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\"../models/arcface_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Находим в базе наибольшего похожего человека:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_FallbackException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/code/opencv/.venv/lib/python3.10/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(input, paddings, name)\u001b[0m\n\u001b[1;32m   6810\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6811\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6812\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   6813\u001b[0m         _ctx, \"Pad\", name, input, paddings)\n",
      "\u001b[0;31m_FallbackException\u001b[0m: This function does not handle the case of the path where all inputs are not already EagerTensors.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13907/822404820.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mimg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mimg2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mimg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mimg2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     obj = DeepFace.verify(img1, img2\n\u001b[0m\u001b[1;32m      9\u001b[0m     , model_name = 'ArcFace', distance_metric = 'euclidean', enforce_detection=False)\n\u001b[1;32m     10\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"verified\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/opencv/.venv/lib/python3.10/site-packages/deepface/DeepFace.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(img1_path, img2_path, model_name, detector_backend, distance_metric, enforce_detection, align, expand_percentage, normalization)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;34m-\u001b[0m \u001b[0;34m'time'\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTime\u001b[0m \u001b[0mtaken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mverification\u001b[0m \u001b[0mprocess\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \"\"\"\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     return verification.verify(\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mimg1_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg1_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mimg2_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg2_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/opencv/.venv/lib/python3.10/site-packages/deepface/modules/verification.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(img1_path, img2_path, model_name, detector_backend, distance_metric, enforce_detection, align, expand_percentage, normalization)\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0malign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malign\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0mnormalization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnormalization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             )\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             img2_embedding_obj = representation.represent(\n\u001b[0m\u001b[1;32m    128\u001b[0m                 \u001b[0mimg_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg2_content\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0menforce_detection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menforce_detection\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/opencv/.venv/lib/python3.10/site-packages/deepface/modules/representation.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(img_path, model_name, enforce_detection, detector_backend, align, expand_percentage, normalization)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mconfidence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_obj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"confidence\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;31m# custom normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnormalization\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mresp_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mresp_obj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"embedding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/opencv/.venv/lib/python3.10/site-packages/deepface/basemodels/ArcFace.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0membeddings\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmulti\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdimensional\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \"\"\"\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m# model.predict causes memory issue when it is called in a for loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# embedding = model.predict(img, verbose=0)[0].tolist()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/code/opencv/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/code/opencv/.venv/lib/python3.10/site-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    586\u001b[0m                 \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mcopied_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcopied_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0mlayout_map_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_subclass_model_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layout_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/code/opencv/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/code/opencv/.venv/lib/python3.10/site-packages/keras/src/engine/base_layer.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m                 with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1147\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m                 ):\n\u001b[0;32m-> 1149\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/opencv/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/code/opencv/.venv/lib/python3.10/site-packages/keras/src/engine/functional.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0mReturns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthere\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0msingle\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtensors\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mthan\u001b[0m \u001b[0mone\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \"\"\"\n\u001b[0;32m--> 515\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_internal_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/code/opencv/.venv/lib/python3.10/site-packages/keras/src/engine/functional.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    668\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_id\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensor_dict\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_input_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                     \u001b[0;32mcontinue\u001b[0m  \u001b[0;31m# Node is not computable, try skipping.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m                 \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m                 for x_id, y in zip(\n",
      "\u001b[0;32m~/code/opencv/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/code/opencv/.venv/lib/python3.10/site-packages/keras/src/engine/base_layer.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m                 with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1147\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m                 ):\n\u001b[0;32m-> 1149\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/opencv/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/code/opencv/.venv/lib/python3.10/site-packages/keras/src/layers/reshaping/zero_padding2d.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         return backend.spatial_2d_padding(\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n",
      "\u001b[0;32m~/code/opencv/.venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/code/opencv/.venv/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1261\u001b[0;31m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1262\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m         \u001b[0;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/opencv/.venv/lib/python3.10/site-packages/keras/src/backend.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(x, padding, data_format)\u001b[0m\n\u001b[1;32m   4035\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"channels_first\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4036\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4037\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4038\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4039\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/code/opencv/.venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/code/opencv/.venv/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1261\u001b[0;31m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1262\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m         \u001b[0;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/opencv/.venv/lib/python3.10/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(tensor, paddings, mode, name, constant_values)\u001b[0m\n\u001b[1;32m   3564\u001b[0m     \u001b[0;31m# remove the \"Pad\" fallback here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3565\u001b[0m     if (not tensor_util.is_tf_type(constant_values) and\n\u001b[1;32m   3566\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconstant_values\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3567\u001b[0m         constant_values == np.zeros_like(constant_values)):\n\u001b[0;32m-> 3568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3569\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3570\u001b[0m       result = gen_array_ops.pad_v2(\n\u001b[1;32m   3571\u001b[0m           tensor, paddings, constant_values, name=name)\n",
      "\u001b[0;32m~/code/opencv/.venv/lib/python3.10/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(input, paddings, name)\u001b[0m\n\u001b[1;32m   6808\u001b[0m   \u001b[0m_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6809\u001b[0m   \u001b[0mtld\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6810\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6811\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6812\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   6813\u001b[0m         _ctx, \"Pad\", name, input, paddings)\n\u001b[1;32m   6814\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6815\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "actuals = []; predictions = []\n",
    "for i in range(0, pairs.shape[0]):\n",
    "    pair = pairs[i]\n",
    "    img1 = pair[0]\n",
    "    img2 = pair[1]\n",
    "    img1 = img1[:,:,::-1]\n",
    "    img2 = img2[:,:,::-1]\n",
    "    obj = DeepFace.verify(img1, img2\n",
    "    , model_name = 'ArcFace', distance_metric = 'euclidean', enforce_detection=False)\n",
    "    prediction = obj[\"verified\"]\n",
    "    predictions.append(prediction)\n",
    "\n",
    "    actual = True if labels[i] == 1 else False\n",
    "    actuals.append(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "accuracy = 100*accuracy_score(actuals, predictions)\n",
    "precision = 100*precision_score(actuals, predictions)\n",
    "recall = 100*recall_score(actuals, predictions)\n",
    "f1 = 100*f1_score(actuals, predictions)\n",
    "print(precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0 500]\n",
      " [  0 500]]\n",
      "0 500 0 500\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(actuals, predictions)\n",
    "print(cm)\n",
    " \n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(tn, fp, fn, tp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
