{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Алгоритм глубокого обучения с использованием рекуррентных нейронных сетей (RNN) для распознавания лиц\n",
    "\n",
    "## 1. Краткий обзор\n",
    "Рекуррентные нейронные сети (RNN) являются мощным инструментом для обработки последовательных данных, таких как временные ряды, текст и видео. В контексте распознавания лиц RNN могут быть использованы для анализа последовательностей изображений или видео кадров, где временные зависимости между кадрами могут улучшить точность распознавания. В частности, Long Short-Term Memory (LSTM) и Gated Recurrent Units (GRU) — это усовершенствованные варианты RNN, которые решают проблему исчезающих и взрывающихся градиентов.\n",
    "\n",
    "## 2. Цель метода\n",
    "Цель использования RNN в распознавании лиц заключается в эффективной обработке и анализе последовательных данных, чтобы учитывать временные зависимости и контекстную информацию, что может улучшить точность и надежность распознавания лиц.\n",
    "\n",
    "## 3. Принцип работы алгоритма\n",
    "RNN обрабатывают последовательные данные, используя скрытые состояния, которые обновляются на каждом временном шаге. Это позволяет сети сохранять информацию о предыдущих входах и использовать её для текущих предсказаний.\n",
    "\n",
    "Основные шаги работы RNN:\n",
    "\n",
    "1. **Входные данные:** Входные последовательности (например, кадры видео) подаются в сеть.\n",
    "2. **Скрытые состояния:** На каждом временном шаге входное значение и предыдущее скрытое состояние используются для вычисления нового скрытого состояния.\n",
    "  $\n",
    "   h_t = \\sigma(W_{xh} x_t + W_{hh} h_{t-1} + b_h)\n",
    "  $\n",
    "   где $h_t$ - скрытое состояние в момент времени $t$, $x_t$ - входное значение, $W_{xh}$ и $W_{hh}$ - весовые матрицы, $b_h$ - смещение, $\\sigma$ - функция активации.\n",
    "3. **Выходные данные:** Скрытые состояния используются для вычисления выходов сети.\n",
    "  $\n",
    "   y_t = \\sigma(W_{hy} h_t + b_y)\n",
    "  $\n",
    "   где $y_t$ - выходное значение, $W_{hy}$ - весовая матрица, $b_y$ - смещение, $\\sigma$ - функция активации.\n",
    "\n",
    "## 4. Схема работы алгоритма\n",
    "```plaintext\n",
    "1. Input Sequence (Video Frames) -> 2. RNN Layers (LSTM/GRU) -> 3. Hidden States -> 4. Output Sequence (Face Recognition Predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Эксперимент на открытой, готовой библиотеке\n",
    "Пример кода для использования LSTM для распознавания лиц с использованием Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Davron\\code\\seller_bot\\.venv\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.2065 - loss: 1.6273 - val_accuracy: 0.1786 - val_loss: 1.6123\n",
      "Epoch 2/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1922 - loss: 1.6166 - val_accuracy: 0.1429 - val_loss: 1.6214\n",
      "Epoch 3/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2233 - loss: 1.6048 - val_accuracy: 0.1429 - val_loss: 1.6278\n",
      "Epoch 4/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2584 - loss: 1.5943 - val_accuracy: 0.1929 - val_loss: 1.6388\n",
      "Epoch 5/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2556 - loss: 1.5994 - val_accuracy: 0.1429 - val_loss: 1.6227\n",
      "Epoch 6/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2081 - loss: 1.6031 - val_accuracy: 0.1571 - val_loss: 1.6173\n",
      "Epoch 7/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.2761 - loss: 1.5940 - val_accuracy: 0.1500 - val_loss: 1.6319\n",
      "Epoch 8/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2759 - loss: 1.5825 - val_accuracy: 0.1714 - val_loss: 1.6514\n",
      "Epoch 9/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2743 - loss: 1.5770 - val_accuracy: 0.1786 - val_loss: 1.6773\n",
      "Epoch 10/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2261 - loss: 1.5797 - val_accuracy: 0.2643 - val_loss: 1.6238\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2262 - loss: 1.6035 \n",
      "Accuracy: 0.23\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Генерация фиктивных данных (последовательности изображений лиц)\n",
    "# Здесь мы создаем случайные данные для демонстрации. В реальном случае используйте видеокадры.\n",
    "num_samples = 1000\n",
    "sequence_length = 10\n",
    "num_features = 128  # Примерное количество признаков на один кадр\n",
    "num_classes = 5  # Количество различных лиц\n",
    "\n",
    "X = np.random.rand(num_samples, sequence_length, num_features)\n",
    "y = np.random.randint(0, num_classes, num_samples)\n",
    "\n",
    "# Преобразование меток в категориальные\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y_categorical = to_categorical(y_encoded, num_classes=num_classes)\n",
    "\n",
    "# Разделение данных на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.3, random_state=42)\n",
    "\n",
    "# Создание модели RNN с LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(sequence_length, num_features), return_sequences=True))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Обучение модели\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Оценка модели на тестовых данных\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Преимущества и недостатки\n",
    "#### Преимущества\n",
    "- Учет временных зависимостей: RNN эффективно обрабатывают последовательные данные, что улучшает точность распознавания лиц.\n",
    "- Снижение неопределенности: Использование нечеткой логики уменьшает неопределенность в картах признаков.\n",
    "- Гибкость: RNN могут применяться к различным типам последовательных данных.\n",
    "#### Недостатки\n",
    "- Высокая вычислительная сложность: Обучение и предсказание с использованием CNN-RNN требует значительных вычислительных ресурсов.\n",
    "- Сложность обучения: RNN могут сталкиваться с проблемами исчезающих и взрывающихся градиентов.\n",
    "- Необходимость большого объема данных: Для эффективного обучения требуется большой объем данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Статьи по данной теме:\n",
    "1. [A Novel Fuzzy Optimized CNN-RNN Method for Facial Expression Recognition](https://www.researchgate.net/publication/356960662_A_novel_fuzzy_optimized_cnn-rnn_method_for_facial_expression_recognition)\n",
    "2. [Recurrent Embedding Aggregation Network for Video Face Recognition](https://arxiv.org/pdf/1904.12019)\n",
    "3. [Facial Expression Recognition with Recurrent Neural Networks](https://www.cs.toronto.edu/~graves/cotesys_2008.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
